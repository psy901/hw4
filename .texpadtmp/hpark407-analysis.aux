\relax 
\citation{rl}
\citation{qlearning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Overview}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Target Problems}{1}}
\newlabel{sec:prob}{{2}{1}}
\newlabel{def:preventions}{{2.1}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:easymap}{{1a}{2}}
\newlabel{sub@fig:easymap}{{a}{2}}
\newlabel{fig:hardmap}{{1b}{2}}
\newlabel{sub@fig:hardmap}{{b}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Two MDPs problems in this report. The gray circle in the bottom left is the agent. The blue box in the upper right is the treasure. The red box in the upper right is the trap. The black boxes are walls. \relax }}{2}}
\newlabel{fig:map}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results and Analysis}{2}}
\newlabel{sec:exp}{{3}{2}}
\citation{plan}
\citation{time}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Optimized Policy}{3}}
\newlabel{sec:policy}{{3.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Optimization Process}{3}}
\newlabel{sec:process}{{3.2}{3}}
\newlabel{fig:easy-vi}{{2a}{4}}
\newlabel{sub@fig:easy-vi}{{a}{4}}
\newlabel{fig:easy-pi}{{2b}{4}}
\newlabel{sub@fig:easy-pi}{{b}{4}}
\newlabel{fig:easy-q}{{2c}{4}}
\newlabel{sub@fig:easy-q}{{c}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Optimal policies of the plannings and reinforcement learning algorithms, given easy grid world problem. Gray boxes indicate walls. The green boxes indicate the treasure. The red boxes indicate the trap. Other boxes are normal blocks. Their colors indicate rewards; being closer to blues means higher rewards. The arrows mean the best action on the corresponding state. \relax }}{4}}
\newlabel{fig:policy-easy}{{2}{4}}
\newlabel{fig:hard-vi}{{3a}{4}}
\newlabel{sub@fig:hard-vi}{{a}{4}}
\newlabel{fig:hard-pi}{{3b}{4}}
\newlabel{sub@fig:hard-pi}{{b}{4}}
\newlabel{fig:hard-q}{{3c}{4}}
\newlabel{sub@fig:hard-q}{{c}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Optimal policies of the plannings and reinforcement learning algorithms, given hard grid world problem. Gray boxes indicate walls. The white boxes with green edges and \texttt  {v} symbol indicate the treasure. The white boxes with red edges and \texttt  {x} symbol indicate the trap. Other boxes are normal blocks. Their colors indicate rewards; being closer to blues means higher rewards. The arrows mean the best action on the corresponding state. \relax }}{4}}
\newlabel{fig:policy-hard}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Result of Q-learning where hard grid world is given. This is the same with Figure (3c\hbox {}). One can observe that q-learning is vulnerable to the dead end and prison; the direction of the optimal transition is arranged to be stuck at the dead end and the prison. \relax }}{5}}
\newlabel{fig:policy-hard-q}{{4}{5}}
\newlabel{fig:easy-vi-reward}{{5a}{5}}
\newlabel{sub@fig:easy-vi-reward}{{a}{5}}
\newlabel{fig:easy-pi-reward}{{5b}{5}}
\newlabel{sub@fig:easy-pi-reward}{{b}{5}}
\newlabel{fig:easy-q-reward}{{5c}{5}}
\newlabel{sub@fig:easy-q-reward}{{c}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The total reward gained for the optimal policy given the number of iterations the algorithm was run in easy grid world problem. \relax }}{5}}
\newlabel{fig:reward-easy}{{5}{5}}
\newlabel{fig:hard-vi-reward}{{6a}{5}}
\newlabel{sub@fig:hard-vi-reward}{{a}{5}}
\newlabel{fig:hard-pi-reward}{{6b}{5}}
\newlabel{sub@fig:hard-pi-reward}{{b}{5}}
\newlabel{fig:hard-q-reward}{{6c}{5}}
\newlabel{sub@fig:hard-q-reward}{{c}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The total reward gained for the optimal policy given the number of iterations the algorithm was run in the hard grid world problem. \relax }}{5}}
\newlabel{fig:reward-hard}{{6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Execution Time}{6}}
\newlabel{sec:time}{{3.3}{6}}
\newlabel{fig:easy-vi-time}{{7a}{6}}
\newlabel{sub@fig:easy-vi-time}{{a}{6}}
\newlabel{fig:hard-pi-time}{{7b}{6}}
\newlabel{sub@fig:hard-pi-time}{{b}{6}}
\newlabel{fig:easy-q-time}{{7c}{6}}
\newlabel{sub@fig:easy-q-time}{{c}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  The total time of the algorithms given the number of iterations the algorithm was run in the hard grid world problem. \relax }}{6}}
\newlabel{fig:time-easy}{{7}{6}}
\newlabel{fig:hard-vi-time}{{8a}{6}}
\newlabel{sub@fig:hard-vi-time}{{a}{6}}
\newlabel{fig:hard-pi-time}{{8b}{6}}
\newlabel{sub@fig:hard-pi-time}{{b}{6}}
\newlabel{fig:hard-q-time}{{8c}{6}}
\newlabel{sub@fig:hard-q-time}{{c}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  The total time of the algorithms given the number of iterations the algorithm was run in the hard grid world problem. \relax }}{6}}
\newlabel{fig:time-hard}{{8}{6}}
\newlabel{fig:trap}{{9a}{7}}
\newlabel{sub@fig:trap}{{a}{7}}
\newlabel{fig:notrap}{{9b}{7}}
\newlabel{sub@fig:notrap}{{b}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Comparison of the total time of the algorithms given the number of iterations the algorithm was run in the hard grid world problem. Figure 9a\hbox {} shows the result where the trap, the red box in the map, exists. Figure 9b\hbox {} shows the result where the trap does not exist. \relax }}{7}}
\newlabel{fig:time-traps}{{9}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{7}}
\newlabel{sec:discuss}{{4}{7}}
\bibstyle{unsrt}
\bibdata{ref}
\newlabel{eq:vi}{{1}{8}}
\newlabel{eq:pi-eval}{{2}{8}}
\newlabel{eq:pi-imp}{{3}{8}}
\newlabel{eq:q}{{4}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}}
\newlabel{sec:conclusion}{{5}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix: How to Run Code}{8}}
\newlabel{sec:app}{{6}{8}}
\bibcite{rl}{{1}{}{{}}{{}}}
\bibcite{qlearning}{{2}{}{{}}{{}}}
\bibcite{plan}{{3}{}{{}}{{}}}
\bibcite{time}{{4}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
